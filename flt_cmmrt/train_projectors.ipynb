{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1f53dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernesto12/.local/lib/python3.10/site-packages/tqdm-4.65.0-py3.10.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "from cmmrt.projection.data import _load_predret_with_xabier_predictions, load_predret_with_predictions\n",
    "from cmmrt.projection.projection_tasks import ProjectionsTasks\n",
    "from cmmrt.projection.metatrainers.utils import create_metatrainer\n",
    "from cmmrt.projection.models.utils import create_projector_and_optimizer\n",
    "from cmmrt.utils.generic_utils import handle_saving_dir\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19f24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser():\n",
    "    \"\"\"Command line parser for the meta-training and meta-testing scripts.\"\"\"\n",
    "    import argparse\n",
    "    my_parser = argparse.ArgumentParser(description='meta-train gp on projection tasks')\n",
    "    my_parser.add_argument('--dataset', type=str, default='cmm')\n",
    "    my_parser.add_argument('--direction', type=str, default='p2e',\n",
    "                           help='p2e (predicted to experimental) or e2p (experimental to predicted)')\n",
    "    my_parser.add_argument('--epochs', type=int, default=-1,\n",
    "                           help='Number of epochs for meta-train the gp. Use -1 '\n",
    "                                'to denote \"until convergence\"')\n",
    "    my_parser.add_argument('--inner_epochs', type=int, default=10)\n",
    "    my_parser.add_argument('--device', type=str, default='cpu')\n",
    "    my_parser.add_argument('--mean', type=str, default='zero')  # mlp_mean or not\n",
    "    my_parser.add_argument('--kernel', type=str, default='rbf+linear')\n",
    "    my_parser.add_argument('--save_to', type=str, default='.', help='folder where to save the figures/results')\n",
    "    my_parser.add_argument('--weight_decay', type=float, default=4e-3)\n",
    "    my_parser.add_argument('--seed', type=int, default=0)\n",
    "\n",
    "    return my_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e762d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_models(args):\n",
    "    projector, inner_optimizer = create_projector_and_optimizer(\n",
    "        model_type='exact',\n",
    "        mean=args.mean,\n",
    "        kernel=args.kernel,\n",
    "        lr=1e-3,\n",
    "        weight_decay=args.weight_decay,\n",
    "        device=args.device\n",
    "    )\n",
    "    metatrainer = create_metatrainer(\n",
    "        metatrainer_name='naive',\n",
    "        projector=projector,\n",
    "        inner_optimizer=inner_optimizer,\n",
    "        inner_epochs=args.inner_epochs,\n",
    "        outer_epochs=args.epochs,\n",
    "        device=args.device\n",
    "    )\n",
    "    return projector, inner_optimizer, metatrainer\n",
    "\n",
    "\n",
    "def get_basename(args):\n",
    "    \"\"\"Use command line arguments to create a basename for the results\"\"\"\n",
    "    filename = args.direction + \"_\"\n",
    "    filename += args.mean\n",
    "    filename += ('_' + args.kernel)\n",
    "    filename += \"_{:.0e}\".format(args.weight_decay).replace('-', '')\n",
    "    filename += f\"_{args.epochs}_{args.inner_epochs}\"\n",
    "    filename = os.path.join(args.save_to, filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def add_timestamp(filename):\n",
    "    \"\"\"Add timestamp to filename\"\"\"\n",
    "    import time\n",
    "    timestamp = str(int(round(time.time() * 1000)))\n",
    "    return filename + f\"-{timestamp}\"\n",
    "\n",
    "\n",
    "def split_systems_on_train_test(data, direction, x_scaler, y_scaler,\n",
    "                                systems=[\"FEM_long\", \"LIFE_old\", \"FEM_orbitrap_plasma\", \"RIKEN\"]):\n",
    "    train_data = data[~data.system.isin(systems)]\n",
    "    test_data = data[data.system.isin(systems)]\n",
    "    train_tasks = ProjectionsTasks(train_data, direction, (1.0, 1.0), min_n=20, x_scaler=x_scaler, y_scaler=y_scaler)\n",
    "    if test_data.empty:\n",
    "        test_tasks = None\n",
    "    else:\n",
    "        test_tasks = ProjectionsTasks(test_data, direction, (1.0, 1.0), min_n=20, x_scaler=x_scaler, y_scaler=y_scaler)\n",
    "    return train_tasks, test_tasks\n",
    "\n",
    "\n",
    "def load_train_test_tasks(dataset, direction, download_directory, remove_non_retained, test_systems):\n",
    "    data, systems, x_scaler, y_scaler = load_predret_with_predictions(dataset, download_directory, remove_non_retained)\n",
    "    train_tasks, test_tasks = split_systems_on_train_test(data, direction, x_scaler, y_scaler, test_systems)\n",
    "    return train_tasks, test_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa42433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_parser = create_parser()\n",
    "args = my_parser.parse_args([\n",
    "    '--save_to', 'results/train_projectors',\n",
    "    '--epochs', '-1'\n",
    "])\n",
    "handle_saving_dir(args.save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580fb4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Predret with cmm predictions...\n"
     ]
    }
   ],
   "source": [
    "train_tasks, test_tasks = load_train_test_tasks(\n",
    "        args.dataset,\n",
    "        args.direction,\n",
    "        download_directory=\"rt_data\",\n",
    "        remove_non_retained=True,\n",
    "        test_systems=['']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19966f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param length: 5\n",
      "[10] - Loss: 0.620\n",
      "[20] - Loss: 0.607\n",
      "[30] - Loss: 0.606\n",
      "[40] - Loss: 0.606\n",
      "[50] - Loss: 0.606\n",
      "[60] - Loss: 0.606\n",
      "[70] - Loss: 0.606\n",
      "[80] - Loss: 0.606\n",
      "[90] - Loss: 0.606\n",
      "[100] - Loss: 0.606\n",
      "[110] - Loss: 0.606\n",
      "[120] - Loss: 0.606\n",
      "[130] - Loss: 0.606\n",
      "[140] - Loss: 0.606\n",
      "[150] - Loss: 0.606\n",
      "[160] - Loss: 0.605\n",
      "[170] - Loss: 0.602\n",
      "[180] - Loss: 0.592\n",
      "[190] - Loss: 0.592\n",
      "[200] - Loss: 0.592\n",
      "[210] - Loss: 0.591\n",
      "[220] - Loss: 0.591\n",
      "[230] - Loss: 0.590\n",
      "[240] - Loss: 0.590\n",
      "[250] - Loss: 0.590\n",
      "[260] - Loss: 0.589\n",
      "[270] - Loss: 0.589\n",
      "Model has converged! Stopping training at epoch 275\n",
      "Done meta-training\n"
     ]
    }
   ],
   "source": [
    "projector, inner_optimizer, metatrainer = create_meta_models(args)\n",
    "projector.prepare_metatraining()\n",
    "metatrainer.metatrain(train_tasks)\n",
    "torch.save(projector.state_dict(), get_basename(args))\n",
    "print('Done meta-training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
